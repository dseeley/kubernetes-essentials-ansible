---

- name: node/hostconfig
  block:
    - name: node/hostconfig | Disable swap for current session
      command: swapoff -a
      become: yes

    - name: node/hostconfig | Remove swapfile from /etc/fstab
      become: yes
      mount:
        name: "{{ item }}"
        fstype: swap
        state: absent
      with_items:
        - swap
        - none

    - name: node/hostconfig | Revert hostname to AWS default - needed because kubelet needs to be able to resolve by hostname (and AWS internal DNS)
      become: yes
      hostname:
        name: "ip-{{ansible_default_ipv4.address.split('.') | join('-')}}"
      when: cluster_vars.type == "aws"

    - name: node/hostconfig | set search domain
      block:
        - name: node/hostconfig | Add search domain to dhclient.conf (so resolv.conf survives reboot)
          blockinfile:
            dest: /etc/dhcp/dhclient.conf
            block: |
              interface "eth0" {
                  append domain-search {{ '"' + search_domain|join('", "') + '"'}};
              }
            state: present
            insertbefore: "BOF"
            create: yes
            backup: yes
            marker: "# Ansible search domain {mark}"

        - name: node/hostconfig | add dns search domain to resolv.conf
          lineinfile:
            path: /etc/resolv.conf
            regexp: '^search.*'
            line: 'search {{search_domain | join(" ")}}'
      become: yes
      vars:
        search_domain: ["default.svc.{{k8s_network.cluster_tld}}", "svc.{{k8s_network.cluster_tld}}", "{{k8s_network.cluster_tld}}", "{% if cluster_vars.dns_cloud_internal_domain is defined and cluster_vars.dns_cloud_internal_domain != '' -%}{{cluster_vars.dns_cloud_internal_domain}}{% endif -%}", "{% if cluster_vars.dns_nameserver_zone is defined and cluster_vars.dns_nameserver_zone != '' -%}{{cluster_vars.dns_nameserver_zone}}{% endif -%}"]


- name: node | Run cloud-specific VIP config (if defined)
  include_tasks: "{{item}}"
  with_first_found: [{ files: ["vips_{{cluster_vars.type}}.yml"], skip: true }]

- name: node | install node-dependent packages
  become: yes
  apt:
    update_cache: yes
    name: ['conntrack', 'ipset']
    state: present

- name: node | install container runtime
  include_tasks: config-containerd.yml

- name: node | install and configure kubelet
  include_tasks: config-kubelet.yml

- name: node | install container kube-proxy
  include_tasks: config-kube-proxy.yml

- name: node | flush handlers
  meta: flush_handlers

- name: node | Wait for nodes to be ready
  uri:
    url: "{% if apiserver.url is defined and apiserver.url != '' %}{{ apiserver.url }}{% elif apiserver.keepalived.vip_cidr is defined and apiserver.keepalived.vip_cidr != '' %}https://{{apiserver.keepalived.vip_cidr | ipaddr('address')}}{% endif %}:{{apiserver.secure_port}}/api/v1/nodes/"
    method: GET
    status_code: 200
    validate_certs: no
    ca_path: "{{k8s_home_kube_dir}}/{{cluster_name}}/ca-crt.pem"
    client_cert: "{{k8s_home_kube_dir}}/{{cluster_name}}/admin-crt.pem"
    client_key: "{{k8s_home_kube_dir}}/{{cluster_name}}/admin-key.pem"
    return_content: yes
  delegate_to: localhost
  run_once: yes
  register: r__clusternodes_result
  until: "r__clusternodes_result.status == 200  and  'json' in r__clusternodes_result  and  (not r__clusternodes_result.json['items'] | json_query(\"[].metadata.name\") | symmetric_difference(play_hosts))"
  retries: 60     #5 minutes
  delay: 5

- name: node/worker | label all nodes with 'node' role
  delegate_to: localhost
  k8s:
    kubeconfig: "{{k8s_home_kube_dir}}/config"
    validate: { fail_on_error: yes }
    resource_definition:
      kind: Node
      metadata:
        name: "{{ansible_hostname}}"
        labels:
          node-role.kubernetes.io/node: 'true'

- name: node/edge | label and taint node-edge nodes with 'edge_ingress.node_role_name' role
  delegate_to: localhost
  k8s:
    kubeconfig: "{{k8s_home_kube_dir}}/config"
    validate: { fail_on_error: yes }
    resource_definition:
      kind: Node
      metadata:
        name: "{{ansible_hostname}}"
        labels: "{{ { 'node-role.kubernetes.io/' + edge_ingress.node_role_name: 'true' } }}"
      spec:
        taints:
          - key: "node-role.kubernetes.io/{{edge_ingress.node_role_name}}"
            value: "true"
            effect: NoExecute
  when: groups['node-edge'] is defined and ansible_hostname in groups['node-edge']


- name: node/edge/controller/haproxy-ingress | apply config
  k8s:
    kubeconfig: "{{k8s_home_kube_dir}}/config"
    apply: yes
    validate: { fail_on_error: yes }
    resource_definition: "{{ item | from_yaml }}"
  loop: "{{ lookup('template', 'templates/haproxy-ingress.yml.j2').split('---') | select() }}"
  delegate_to: localhost
  run_once: true
  when: edge_ingress.controller == "haproxy-ingress"

- name: node/edge/vip/keepalived-vip | apply config
  k8s:
    kubeconfig: "{{k8s_home_kube_dir}}/config"
    apply: yes
    validate: { fail_on_error: yes }
    resource_definition: "{{ item | from_yaml }}"
  loop: "{{ lookup('template', 'templates/kube-keepalived-vip.yaml.j2').split('---') | select() }}"
  delegate_to: localhost
  run_once: true
  when: edge_ingress.ha_mgr == "kube-keepalived-vip"  and  edge_ingress.keepalived.vip_cidr is defined and edge_ingress.keepalived.vip_cidr != ""


#- name: node/edge/controller/gcp | Install application loadbalancer
#  include_tasks: gce-lb-apps.yml
#  when: edge_ingress.controller == "gcp_external"
